

핵심은 **"이겼을 때와 졌을 때의 계산 방식이 다른, 이중적인(Dual) Loss Function"**을 사용한다는 점입니다. 이를 통계학 용어로는 **NLL (Negative Log-Likelihood)**이라고 합니다.


---

## 1. 기본 개념: 왜 MSE를 못 쓸까?

일반적인 딥러닝(예: 집값 예측)에서는 정답()과 예측()의 차이를 줄이는 MSE를 씁니다.
하지만 패찰(Loss) 데이터는 **정답(, 1등 가격)을 모릅니다.**

* **승리 (Win):** 정답 원. (정확히 암)
* **패배 (Loss):** 정답 원. (80원보다는 크다는 것만 암)

이 **"80원보다 크다"**라는 불확실한 정보를 학습에 반영하기 위해 **확률 밀도(Likelihood)**를 최대화하는 방식을 씁니다.

---

## 2. Loss Function의 구성 (The Formula)

전체 Loss는 **승리한 데이터의 Loss()**와 **패배한 데이터의 Loss()**의 합으로 정의됩니다.

### A. 이겼을 때 (Winning Case): PDF 사용

내가 원에 낙찰받았다면, 모델이 만든 확률 분포(GMM)에서 ** 지점의 확률 밀도(높이)**가 높게 나와야 합니다.

* **목표:** 확률 밀도 함수(PDF) 값 $P(z|x)$를 최대화하자.
* **식:** 
* **직관:** "내 모델이 그린 종 모양의 그래프에서, 실제 낙찰가()가 꼭대기 근처에 위치하게 해라."

### B. 졌을 때 (Losing Case): CDF 사용

내가 원을 썼는데 졌다면, 진짜 1등 가격()은 **보다 오른쪽에 있다**는 뜻입니다.

* **목표:** 보다 큰 구간의 **확률 면적(Area)**을 최대화하자.
* **식:** 
* : 원 이하일 확률 (왼쪽 면적)
* : 원 초과일 확률 (오른쪽 면적, **Survival Function**)


* **직관:** "내 모델이 그린 그래프에서, **원보다 비싼 쪽의 꼬리 면적**이 넓어야 한다. (그래야 내가 진 게 말이 되니까)"

---

## 3. 최종 수식 (GMM과 결합)

논문에서는 모델이 GMM(여러 정규분포의 합)이므로, PDF와 CDF를 아래와 같이 풀어서 씁니다.

* : 뉴럴 네트워크가 뱉어낸 출력값들
* : 정규분포 함수 (PDF)
* : 누적 정규분포 함수 (CDF)
* : 승리했을 때의 실제 낙찰가
* : 패배했을 때의 나의 입찰가

---

## 4. 이 Loss 설정의 효과 (시각적 이해)

이 방식을 쓰면 모델이 어떻게 똑똑해지는지 그림으로 상상해 보세요.

1. **패배 데이터()가 들어옴:**
* 모델이 처음에 "낙찰가는 평균 400원일 거야"라고 분포를 그리고 있었음.
* Loss 계산: "어? 500원 입찰했는데 졌다고? 그럼 진짜 가격은 500원보다 커야지!" ($1-CDF(500)$을 키워라!)
* **Backpropagation:** 모델이 분포를 **오른쪽(더 비싼 쪽)으로 밀어버림.**


2. **결과:**
* 패찰 데이터들을 통해 **"보이지 않는 고가 영역"**을 추론해냄.
* 단순히 승리 데이터만 썼을 때 발생하는 **가격 과소평가(Bias)** 문제를 해결함.



---

### 요약

1. **목적:** Observed(승리) 데이터와 Censored(패배) 데이터를 동시에 학습시키기 위함.
2. **방법:** **Negative Log-Likelihood (NLL)** 사용.
3. **구현:**
* **이겼을 땐:** 특정 지점()의 **그래프 높이(PDF)**를 키우는 방향으로.
* **졌을 땐:** 입찰가()보다 **오른쪽의 면적(1-CDF)**을 키우는 방향으로.



이 Loss를 미분해서 딥러닝 모델(MDN)의 파라미터()를 업데이트하게 됩니다. 아주 우아한 방식이죠!
